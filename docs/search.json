[
  {
    "objectID": "Day2_IRB25_plots.html",
    "href": "Day2_IRB25_plots.html",
    "title": "Visualization",
    "section": "",
    "text": "ggplot2 is a powerful and widely used R package for creating elegant, layered, and customizable graphics. It is based on the Grammar of Graphics, which provides a structured approach to building plots by combining data, aesthetics, and geometric objects.\nTo use ggplot2, load the package:\n\nlibrary(ggplot2)\n\n\n\n\n\nplot(iris$Sepal.Length, \n     iris$Petal.Length, \n     col=iris$Species, \n     main = \"Base R\")\n\n\n\nggplot(iris, aes(Sepal.Length, Petal.Length, color=Species)) + \n  geom_point()+\n  theme_light()+\n  ggtitle(\"ggplot2\") +\n  theme(plot.title = element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nEvery plot in ggplot2 starts with the ggplot() function, which defines the dataset and the aesthetic mappings (aes()). You then add one or more geometric layers (geoms), such as:\n\ngeom_point() for scatter plots\ngeom_line() for line plots\ngeom_bar() for bar charts\n\nThese layers are added using the + operator, allowing you to build plots step by step.\nThe aes() function is how data is stored, how data is split, and geoms is what the data looks like. These are geometrical objects stored in subsequent layers.\nAn empty ggplot object is the foundation of any plot. You can build it step by step by specifying the data and aesthetic mappings.\nHere are a few ways to define an empty plot object:\n\n# Empty canvas\np <- ggplot()\n\n# Specify data only\np <- ggplot(data_frame)\n\n# Specify data and aesthetics (x and y mappings)\np <- ggplot(data_frame, aes(x, y))\n\n\n\nExample: Scatter plot\n\n# Create a scatter plot of Sepal.Length vs. Sepal.Width\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point()\n\n\n\n\nThis plot uses the iris dataset and shows the relationship between sepal length and width. The aes() function defines the mapping from data columns to visual properties (x and y axes, color, size, etc.).\n\n\n\n\n\n\nImportant\n\n\n\nYou can keep adding layers (like titles, colors, or trend lines) using the + operator.\n\n\n\n# Create a scatter plot of Sepal.Length vs. Sepal.Width\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title=\"Scatter plot\")\n\n\n\n\n\n\n\nFaceting allows you to split a plot into multiple panels based on the values of one or more categorical variables. This is useful for comparing subgroups side-by-side.\n\n# Scatter plot split by species\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  facet_grid( ~ Species)\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nSyntax Example\n\n\n\n\nfacet_wrap()\nCreates a series of plots wrapped into multiple rows or columns\nfacet_wrap(~ variable)\n\n\nfacet_grid()\nCreates a grid of plots by combining one variable for rows and one for columns\nfacet_grid(row_var ~ col_var)\n\n\nfacet_null()\nDefault; no faceting applied\nfacet_null()\n\n\n\n\n\n\nscales = \"free\": Allows each facet to have its own scale.\nnrow, ncol: Control layout in facet_wrap().\nlabeller: Customize facet labels.\n\nFree y-axis per facet\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  facet_grid(~ Species, scales = \"free_x\")\n\n\n\n\n\n\n\n\nControlling the appearance of plots is key to effective data visualization. In ggplot2, color and fill are used to modify how geometric objects look.\n\n\n\n\n\nAesthetic\nAffects\nCommon in\n\n\n\n\ncolor\nOutline or line color\ngeom_point(), geom_line()\n\n\nfill\nInside (area) color\ngeom_bar(), geom_boxplot()\n\n\n\n\n\n\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot(color = \"black\", fill = \"lightblue\")\n\n\n\n\n\n\n\n\nMapped: Aesthetic is linked to a variable (aes(fill = Species))\nSet manually: Aesthetic is fixed (fill = “skyblue”)\n\n# Mapping\nggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_histogram(bins = 20, position = \"identity\", alpha = 0.6)\n\n\n\n# Setting\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\", bins = 20)\n\n\n\n\n\n\n\nYou can apply your own custom colors using scale_fill_manual() and scale_color_manual().\n\nmy_palette <- c(\"setosa\" = \"#1b9e77\", \"versicolor\" = \"#d95f02\", \"virginica\" = \"#7570b3\")\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3, show.legend = FALSE) +\n  scale_color_manual(values = my_palette ) +\n  facet_grid(~Species)\n\n\n\n\n\n\n\n\n\n\nUseful Tips\n\n\n\n\nUse named vectors to avoid color-order mismatches.\nUse RColorBrewer, viridis, or ggsci for prebuilt, colorblind-friendly palettes.\nUse theme(legend.position = \"none\") to hide legends when needed.\n\n\n\n\n\nThe ggsci package includes curated palettes inspired by scientific journals (e.g., Nature, JAMA) and pop culture (e.g., Star Trek, Rick and Morty).\nYou can switch palettes using functions like:\n\nscale_fill_npg() – Nature Publishing Group\nscale_color_jama() – JAMA\nscale_fill_startrek(), scale_color_rickandmorty() – Fun, themed palettes\n\nIf you do not have the package, you must install it.\n\ninstall.packages(\"ggsci\")\n\n\nlibrary(ggsci)\n\nggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot() +\n  scale_fill_simpsons() +  # NEJM palette\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe ggplot2 package provides a flexible theme() system that allows you to adjust visual aspects of your plots such as fonts, backgrounds, borders, axes, and legend positions. You can use built-in themes or customize your own.\n\ntheme_bw() – clean black-and-white\ntheme_minimal() – light, minimalistic\ntheme_classic() – traditional axes with no grid\ntheme_void() – empty canvas, good for maps\ntheme_light() – lightly gridded version of minimal\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 5) +\n  theme_minimal()\n\n\n\n\nYou can tweak the theme with theme():\n\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point(size = 3) +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        text = element_text(family = \"serif\", size = 12))\n\n\n\n\n\n\n\nYou can overlay multiple geometric layers (geoms) in a single plot to combine their strengths. For example, a violin plot shows the full distribution, while a boxplot shows median, quartiles, and outliers.\n\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_violin(trim = FALSE, alpha = 0.5, aes( fill = Species)) +   # Show full distribution\n  geom_boxplot(width = 0.2, color = \"black\", outlier.shape = NA) +  # Summary stats\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nThe ggpubr package simplifies the process of making polished, publication-ready figures. It includes easy theming, statistical annotations, and tools for combining plots.\nIf you do not have the package, you must install it.\n\ninstall.packages(\"ggpubr\")\n\n\nlibrary(ggpubr)\n\n# Scatter plot with stat_cor (correlation)\nggscatter(iris, x = \"Sepal.Length\", \n          y = \"Petal.Length\",\n          color = \"Species\", \n          add = \"reg.line\", \n          conf.int = TRUE,\n          cor.coef = TRUE, \n          cor.method = \"pearson\") +\n  theme_pubr()\n\n\n\n\nThe ggpubr package makes it easy to create boxplots with group comparisons, including statistical test results directly annotated on the plot.\n\nggboxplot(iris, x = \"Species\", \n          y = \"Sepal.Length\", \n          color = \"Species\", \n          palette = \"jco\",\n          add = \"jitter\") +\n  stat_compare_means()  # Default: Kruskal-Wallis test for >2 groups\n\n\n\n\nAdd pairwise comparisons (e.g., t-tests between groups)\n\nggboxplot(iris, x = \"Species\", \n          y = \"Sepal.Length\", \n          color = \"Species\", \n          palette = \"jco\",\n          add = \"jitter\") +\n  stat_compare_means(comparisons = list(c(\"setosa\", \"versicolor\"), \n                                        c(\"setosa\", \"virginica\"),\n                                        c(\"versicolor\", \"virginica\")),\n                     method = \"t.test\") +\n  stat_compare_means(label.y = 8.5)  # Global p-value (ANOVA/Kruskal-Wallis)\n\n\n\n\n\n\n\nYou can store a ggplot object in a variable and reprint or reuse it later. Using the %+% operator, you can update the underlying dataset without changing the plot structure.\nThis is useful when applying the same plot design to different subsets of data.\n\n# Create a base plot using the full iris dataset\np <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3)\n\n# Print the original plot\nprint(p)\n\n\n\n\nWe now reuse the plot with a filtered dataset containing only virginica species:\n\n# Replace the data with a subset: only virginica species\np2 <- p %+% iris[iris$Species == \"virginica\", ]\n\n# Print the updated plot\nprint(p2)\n\n\n\n\n\n\n\nThe cowplot package provides simple and flexible tools for arranging multiple ggplot2 plots into a single figure. It’s especially useful when preparing figures for publications or presentations.\n\n install.packages(\"cowplot\") \n\n\n# Create two plots using the iris dataset\np1 <- ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Boxplot\")\n\np2 <- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_histogram(bins = 20, alpha = 0.6, position = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Histogram\")\n\n# Combine side by side\ncowplot::plot_grid(p1, p2, labels = c(\"A\", \"B\"))\n\n\n\n\n\n\n\n\n\n\n\n\nArgument\nDescription\nExample\n\n\n\n\n...\nThe plots to combine\nplot_grid(p1, p2)\n\n\nncol\nNumber of columns\nncol = 2\n\n\nnrow\nNumber of rows\nnrow = 1\n\n\nlabels\nAdd labels to subplots (e.g., “A”, “B”, …)\nlabels = c(\"A\", \"B\")\n\n\nlabel_size\nSize of subplot labels\nlabel_size = 14\n\n\nalign\nAlign plots by axis (\"h\", \"v\", \"hv\")\nalign = \"hv\"\n\n\naxis\nShare axis text/limits (\"t\", \"b\", \"l\", \"r\")\naxis = \"tb\"\n\n\nrel_widths\nRelative widths of columns\nrel_widths = c(1, 2)\n\n\nrel_heights\nRelative heights of rows\nrel_heights = c(1, 1.5)\n\n\n\n\ncowplot::plot_grid(p1, p2, ncol = 1, rel_heights = c(1, 2), labels = c(\"A\", \"B\"))\n\n\n\n\n\n\n\nYou can save any ggplot object to a file using the ggsave() function.\n\n\n\n\n\n\n\n\nArgument\nDescription\nExample\n\n\n\n\nfilename\nOutput file name (with extension)\n\"plot.pdf\", \"plot.png\"\n\n\nplot\nPlot object to save (optional)\nplot = p\n\n\nwidth\nWidth in inches\nwidth = 6\n\n\nheight\nHeight in inches\nheight = 4\n\n\ndpi\nResolution (use 300 for print)\ndpi = 300\n\n\n\n\nggsave(\"results/scatterplot.png\", # be careful about the path\n       plot = p1, \n       width = 6, \n       height = 4, \n       dpi = 300)\n\n\n\n\n\n\n\nTip\n\n\n\nFor more information and help with making graphs see the R Graphics Cookbook"
  },
  {
    "objectID": "Day2_IRB25_part1.html",
    "href": "Day2_IRB25_part1.html",
    "title": "Table manipulation",
    "section": "",
    "text": "A data.frame is a 2D table-like structure where columns can hold different data types. data.table is an enhanced version of data.frame that provides faster and more memory-efficient operations.\n\n\n\n\n\n\nBuilt-in datasets\n\n\n\n\nR comes with several built-in datasets that are useful for learning, testing, and demonstrating functions without needing to load external data.\nThese datasets are included in base R and standard packages like datasets, and can be accessed directly by name (e.g., CO2, iris, mtcars, etc.).\n\n\n\nWe will be working with build-in dataset iris introduced by Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems, contains three plant species (setosa, virginica, versicolor) and four features measured for each sample.\nLet’s examine the first six rows of the table!\n\n# Call the table\nhead(iris)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFeature\ndata.frame\ndata.table\n\n\n\n\nBase structure\nBase R object\nExtension of data.frame\n\n\nSpeed\nModerate\nVery fast, optimized for large data\n\n\nSyntax\nVerbose\nConcise and powerful\n\n\nRownames\nSupported\nDiscouraged / ignored\n\n\nMemory usage\nStandard\nMore memory-efficient\n\n\nGrouping\nUses aggregate() or dplyr\nBuilt-in with by=\n\n\nJoining\nUses merge()\nFast joins with keys (setkey())\n\n\nLearning curve\nLow (familiar to most R users)\nMedium (different syntax)\n\n\n\n\n\n\n\n\n\n\n\nFunctions to import the data\n\n\n\n\nbasic R import function:\ndf <- read.csv()\ndf <- read.tsv()\ndf <- read.table()\ndata.table import function:\ndt <- fread()\nusing specialized funcions from R packages to import files such as Excel, SPSS\ndf <- read_excel()\ndf <- read.spss()\n..."
  },
  {
    "objectID": "Day2_IRB25_part1.html#how-to-obtain-data.table",
    "href": "Day2_IRB25_part1.html#how-to-obtain-data.table",
    "title": "Table manipulation",
    "section": "How to obtain data.table?",
    "text": "How to obtain data.table?\n\n\n\n\n\n\nImport as data.table or convert existing formats to data.table\n\n\n\n\nImport data using function fread().\n\ndt_import <- fread(\"PATH/TO/FILE\")\n\nConvert data.frame (even other objects such as matrices, GRanges object…) using as.data.table().\n\ndt_import <- as.data.table(df)\n\nAnother approach of converting object to data.table using setDT() but without defining the new variable.\n\nsetDT(df)\n\n\n\n\n\n\n\n\nTip\n\n\n\nA file path tells R where to find or save a file.\n\nAbsolute path: The full location starting from the root of your computer.\n\nExample: \"C:/Users/Paula/Documents/data.csv\"\n\nRelative path: A shortcut from the current working directory.\n\nExample: \"data/data.csv\"\n\n\nUse getwd() to check your current working directory.\n💡 Tip: In RStudio Projects, use relative paths so your code works on any computer."
  },
  {
    "objectID": "Day2_IRB25_part1.html#from-data.frame-to-data.table",
    "href": "Day2_IRB25_part1.html#from-data.frame-to-data.table",
    "title": "Table manipulation",
    "section": "From data.frame to data.table",
    "text": "From data.frame to data.table\nRun the code chunk below. If you want, check it out using rownames().\n\n( df_letters <- data.frame( one=1:6, two=2:7, row.names=letters[1:6]) )\n\n\n\n  \n\n\n( dt_letters <- as.data.table(df_letters) )\n\n\n\n  \n\n\n\n\nHow to keep the rownames?\nSet argument keep.rownames to TRUE if you want to keep the rownames as a separate column in data.table.\n\n( dt_letters <- as.data.table(df_letters, keep.rownames=TRUE) )\n\n\n\n  \n\n\n\nInstead of keep.rownames=TRUE, you can specify the name of the column containing the rownames from the data.frame.\nExample:\n\n( dt_letters <- as.data.table(df_letters, keep.rownames=\"letters\") )\n\n\n\n  \n\n\n\n\nTask example: Convert iris to data table using as.data.table\nCreate a variable iris_dt that contains iris data set as data.table object. In this tutorial we are going to compare and analyze the default data set iris as data frame and data table objects.\n\niris_dt <- as.data.table(iris)\n\n\niris\n\n\n\n  \n\n\nrownames(iris)\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\"  \"11\"  \"12\" \n [13] \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\" \n [25] \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\" \n [49] \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\" \n [97] \"97\"  \"98\"  \"99\"  \"100\" \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\"\n[109] \"109\" \"110\" \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\" \"120\"\n[121] \"121\" \"122\" \"123\" \"124\" \"125\" \"126\" \"127\" \"128\" \"129\" \"130\" \"131\" \"132\"\n[133] \"133\" \"134\" \"135\" \"136\" \"137\" \"138\" \"139\" \"140\" \"141\" \"142\" \"143\" \"144\"\n[145] \"145\" \"146\" \"147\" \"148\" \"149\" \"150\"\n\nrownames(iris_dt)\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\"  \"11\"  \"12\" \n [13] \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\" \n [25] \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\" \n [49] \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\" \n [97] \"97\"  \"98\"  \"99\"  \"100\" \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\"\n[109] \"109\" \"110\" \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\" \"120\"\n[121] \"121\" \"122\" \"123\" \"124\" \"125\" \"126\" \"127\" \"128\" \"129\" \"130\" \"131\" \"132\"\n[133] \"133\" \"134\" \"135\" \"136\" \"137\" \"138\" \"139\" \"140\" \"141\" \"142\" \"143\" \"144\"\n[145] \"145\" \"146\" \"147\" \"148\" \"149\" \"150\""
  },
  {
    "objectID": "Day2_IRB25_part1.html#row-subsetting",
    "href": "Day2_IRB25_part1.html#row-subsetting",
    "title": "Table manipulation",
    "section": "Row subsetting",
    "text": "Row subsetting\n\nSyntax comparison\n\n\n\n\n\n\n\n\nOperation\ndata.frame\n_data.table_\n\n\n\n\nSubseting rows\ndf[1:20, ]\ndf[1:20]\n\n\nSubseting rows based on criteria\ndf[df$id < 4, ]\ndf[id < 4]\n\n\n\n\nTask\nSelect all rows in iris_dt with Sepal.Length less than 6.7 and Petal.Length less than 1.2.\n\n# Write the solution here\n\niris_dt[Sepal.Length < 6.7 & Petal.Length < 1.2]\n\n\n\n  \n\n\n\nSelect only setosa species with more than 0.3 Petal.Width.\n\n# Write the solution here\niris_dt[Species == \"setosa\" & Petal.Width < 0.3]"
  },
  {
    "objectID": "Day2_IRB25_part1.html#column-subsetting",
    "href": "Day2_IRB25_part1.html#column-subsetting",
    "title": "Table manipulation",
    "section": "Column subsetting",
    "text": "Column subsetting\n\nSyntax comparison\n\n\n\n\n\n\n\n\nOperation\ndata.frame\n_data.table_\n\n\n\n\nSubseting columns\ndf[, c(1,5)]\ndf[, c(1,5)]\n\n\nSubseting columns based on colnames\ndf[, c(\"id\")]\ndf[,.(id)]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nData.table does not use c() but .() which represents list\n\n\nSelect columns Petal.Length and Petal.Width and first 3 rows\n\n# Write the solution here\n\niris_dt[ 1:3, .(Petal.Length, Petal.Width) ]\n\n\n\n  \n\n\n\n\n\nAdditional methods of subsetting columns in data.table\n\n\n\n\n\n\nTip\n\n\n\n\nUsing a character vector of column names\n\nYou can store column names in a vector and use .. to evaluate it inside data.table.\n\ncnames <- c(\"Petal.Length\", \"Petal.Width\")\n\niris_dt[1:3, ..cnames]\n\n\n\n  \n\n\n\n\nUsing a range of column names\n\nYou can select a range of columns using the : operator between two column names.\n\niris_dt[1:3, Sepal.Length:Petal.Width]\n\n\n\n  \n\n\n\n\nNegative subsetting to exclude columns\n\nUse -() or !() to exclude a range of columns.\n\niris_dt[1:3, -(Sepal.Length:Petal.Width)]\n\n\n\n  \n\n\niris_dt[1:3, !(Sepal.Length:Petal.Length)]\n\n\n\n  \n\n\n\nNote: Sepal.Length:Petal.Width works only in column context—it is not the same as numeric indexing.\n\n\n\nTask\nSelect all rows in iris_dt with Sepal.Width equal to 3.0, but show only columns for Species and Sepal.Width (Do it by using column names!). You may also try other methods of subsetting.\n\n# Write the solution here"
  },
  {
    "objectID": "Day2_IRB25_part1.html#operation-on-columns",
    "href": "Day2_IRB25_part1.html#operation-on-columns",
    "title": "Table manipulation",
    "section": "Operation on columns",
    "text": "Operation on columns\n\nSyntax comparison\n\n\n\n\n\n\n\n\nOperation\ndata.frame\ndata.table\n\n\n\n\nCalculate sum\nsum(df$id)\ndf[, sum(id)]\n\n\nCalculate sum by group\nby(df$id, df$group, sum)\ndf[, sum(id), by=group]\n\n\nCalling column out as a vector\ndf$id\ndf[, id]\n\n\n\nCalculate mean of Sepal.Width for all species.\n\niris_dt[, mean(Sepal.Width)]\n\n[1] 3.057333\n\n\nCalculate mean of Sepal.Width for and for each species\n\niris_dt[, mean(Sepal.Width), by=Species] \n\n\n\n  \n\n\n\nAlso the following syntax works for data.table because in the third part it is always the group operation when calculating something per rows: iris_dt[, mean(Sepal.Width), Species]\n\n\n\n\n\n\nAdding the column immediately when performing the operation in data.table\n\n\n\nSyntax: df[, .(new_col = sum(id) ), by=group]\nExample:\n\niris_dt[, .( mean_col = mean(Sepal.Width) ), by=Species]\n\n\n\n  \n\n\n\n\n\n\n\nThe power of data.table over data.frame\ndata.table offers an elegant and efficient way to perform multiple calculations on columns and save the results — all in a concise syntax.\nExample: Calculate mean, standard deviation, max, min, and count of Sepal.Width by Species, then order by mean. ::: callout-important With data.table, you can chain multiple operations using square brackets [], similar in spirit to the dplyr pipe %>%. :::\n\n\n\n\n\n\n\ndata.frame\ndata.table\n\n\n\n\nmeanic <- by(iris$Sepal.Width,INDICES = iris$Species, mean) sdic <- by(iris$Sepal.Width,INDICES = iris$Species, sd) maxx <- by(iris$Sepal.Width,INDICES = iris$Species, max) minx <- by(iris$Sepal.Width,INDICES = iris$Species, min)\nres_df <- data.frame(as.numeric(meanic), as.numeric(sdic), as.numeric(maxx), as.numeric(minx))\nres_df[order(res_df$as.numeric.meanic.),]\niris_dt[ , .(mean=mean(Sepal.Width), sd=sd(Sepal.Width), min_x= min(Sepal.Width), max_x= max(Sepal.Width)), by=Species][order(mean)]\n\n\n\n\nOutput example\n\niris_dt[ , \n        .(mean=mean(Sepal.Width),\n          sd=sd(Sepal.Width),\n          min_x= min(Sepal.Width),\n          max_x= max(Sepal.Width)),\n        by=Species][order(mean)]\n\n\n\n  \n\n\n\n\nTask\nSelect all rows where Sepal.Length < 6.7 and flower species virginica and calculate mean Petal.Width in iris_dt. Do it with and without chaining.\nWith chaining\n\n# Write the solution here\n\nNo chaining\n\n# Write the solution here\n\n\n\n\nAdding new permanent columns\nAdding one columns in data table is done with := and multiple columns with ‘:=’.\nHere is a new column that contains the maximal sepal width for each species was added.\n\niris_dt[, max_width1 := max(Sepal.Width), Species]\niris_dt[1:3]\n\n\n\n  \n\n\n\nAddition of multiple columns.\n\niris_dt[, \":=\" (max_width2 = max(Sepal.Width),  \n                max_width3 = max(Sepal.Length) ),\n        by=.(Species)]\niris_dt[1:3]\n\n\n\n  \n\n\n\n\nTask: Add a new column\nAdd columns to iris_dt that represent mean and sd of Petal.Width grouped by species. Columns are called meanPW and sdPW.\n\n# Write the solution here\n\niris_dt[, \":=\" (meanPW = mean(Petal.Width),  \n                sdPW = sd(Petal.Width) ),\n        by=.(Species)]\niris_dt[1:3]\n\n\n\n  \n\n\n###how to remove columns :D \niris_dt[, sdPW := NULL]\niris_dt[1:3]\n\n\n\n  \n\n\n\nUse function uniqueN() to check how many unique mean Petal.Width there are in total and by Species group.\n\n# Write the solution here\n\n\n\n\nSpecial symbols in data.table\n.N (an integer containing the number of rows in the group)\n\niris_dt[, .N]\n\n[1] 150\n\n\nCan be used on groups. What is happening here with this subsetting in group?\n\niris_dt[, .N, by=.(Species, Sepal.Length>=4.8)]\n\n\n\n  \n\n\n\n\nTask: :=, .N\nAdd columns to iris_dt that represent number of observations of all rows for which Petal.Length is smaller than 6.5 in iris_dt grouped by species.\n\n# Write the solution here\n\nMoreover, show only the newly added columns in your final results and use na.omit() to remove all missing values from your table\n\n# Write the solution here\n\nOne great benefit of data.table is the ability to sub-assign by reference: Try it: select all rows that have species==“virginica” and rename those Species entries using := to new_virginica.\n\n# Write the solution here"
  },
  {
    "objectID": "Day2_IRB25_part1.html#subset-of-the-original-data.table",
    "href": "Day2_IRB25_part1.html#subset-of-the-original-data.table",
    "title": "Table manipulation",
    "section": "Subset of the original Data.table",
    "text": "Subset of the original Data.table\n.SD (a smaller data.table that is a Subset of the original Data.table for each group)\n.SDcols (subset columns which are then used by .SD)\n\nSelect all columns with .SD. Select only a subset of all columns by .SDcols\n\n\n\n\n\n\n\n\n\n\nSelecting columns can be done by writing column names in .SDcols (Note: It is important to write it as stings in c() )\n\niris_dt[, .SD, .SDcols=c(\"Sepal.Width\", \"Species\")][1:2]\n\n\n\n  \n\n\n\nThis allows powerful calculation on subset of desired columns and groups.\n\n\n\n\n\n\n\n\n\nYou can use lapply() together with .SD (Subset of Data) to apply functions to selected columns within groups. This is particularly useful for performing calculations like mean or sum on a subset of numeric columns.\n\niris_dt[, lapply(.SD, mean), by=Species, .SDcols=1:2]\n\n\n\n  \n\n\n\nApply mean to all numeric columns by species\n\niris_dt[, lapply(.SD, mean), by=Species, .SDcols=is.numeric]\n\n\n\n  \n\n\n\n.SD is amazing for selecting first and last row of certain group since we can imagine every group with selected table as a new separate data table. As well as other operation that can be done on data.table.\n\n\n\n\n\n\n\n\n\n\niris_dt[, .SD[c(1, .N)], by=Species]\n\n\n\n  \n\n\n\n\nTask: .SD\nOrder the results by Petal.Width and select first three (smallest) observations by species. Calculate mean of first three columns for iris_dt for those observations. Do it in one command with chaining.\n\n# Write the solution here"
  },
  {
    "objectID": "Day2_IRB25_part1.html#combining-tables",
    "href": "Day2_IRB25_part1.html#combining-tables",
    "title": "Table manipulation",
    "section": "Combining tables",
    "text": "Combining tables\n\nBinding tables\nHow to “stitch” two or more data objects into one?\n\ntb1 <- data.table(sampleID = c(6:1),\n                  cancer = c(\"Breast\",\"Breast\",\"Brain\",\"Liver\",\"Brain\",\"Pancreas\"))\ntb2 <- data.table(sampleID = c(7, 4, 6, 2, 8), \n                  gender = c(\"F\",\"F\",\"M\",\"F\",\"M\")) \nhead(tb1,3)\n\n\n\n  \n\n\nhead(tb2,3)\n\n\n\n  \n\n\n\nFunction rbind() binds the tables by rows. Try run the code below and see what happens. Why is that?\n\nrbind(tb1, tb2)\n\nError in rbindlist(l, use.names, fill, idcol, ignore.attr): Column 2 ['gender'] of item 2 is missing in item 1. Use fill=TRUE to fill with NA (NULL for list columns), or use.names=FALSE to ignore column names.\n\n\n\nrbind(tb1, tb2, fill=TRUE)\n\n\n\n  \n\n\n\nFunction cbind() binds the tables. Do you notice anything strange when you run the code below?\n\ncbind(tb1, tb2)\n\n\n\n  \n\n\n\n\n\nmerge()\n\n\n\n\n\nSpecify by which column you want to merge by and set the argument all= to TRUE to perform the full join\n\nmerge(tb1, tb2, by=\"sampleID\") #if setkey was used prior then merge(tb1,tb2) works the same\n\n\n\n  \n\n\nmerge(tb1, tb2, by=\"sampleID\", all=TRUE)\n\n\n\n  \n\n\n\nRight and left join\n\nmerge(tb1, tb2, by=\"sampleID\", all.x = TRUE)\n\n\n\n  \n\n\nmerge(tb1, tb2, by=\"sampleID\", all.y = TRUE)"
  },
  {
    "objectID": "Day2_IRB25_part1.html#tidying-the-table",
    "href": "Day2_IRB25_part1.html#tidying-the-table",
    "title": "Table manipulation",
    "section": "Tidying the table",
    "text": "Tidying the table\nTidying a data table often includes renaming columns to make them more informative or consistent.\n\n\n\n\n\n\nTip\n\n\n\nYou can rename columns using names() or colnames().\nnames(dt_cancer) <- vec_names\n\n\n\ndt_cancer <- merge(tb1, tb2, by=\"sampleID\", all=TRUE)\ndt_cancer\n\n\n\n  \n\n\nnames(dt_cancer)[1] <- \"id\"\ndt_cancer\n\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can rename columns using data.table: setnames\nsetnames(dt_cancer, \"old_names\", \"new_names\")\n\n\n\nnames(dt_cancer)\n\n[1] \"id\"     \"cancer\" \"gender\"\n\nsetnames(dt_cancer,  \"id\", \"sampleID\")\nnames(dt_cancer)\n\n[1] \"sampleID\" \"cancer\"   \"gender\"  \n\n\n\nOrdering columns\nSubseting\ndt[,.(col3, col10, col2)]\n\ndt_cancer[,.(gender, sampleID)]\n\n\n\n  \n\n\n\ndata.table: setcolorder()\nsetcolorder(dt_cancer, neworder)\n\ndt_cancer\n\n\n\n  \n\n\nsetcolorder(dt_cancer, c(\"gender\",\"sampleID\"))\ndt_cancer"
  },
  {
    "objectID": "Day2_IRB25_part1.html#reshaping-the-table",
    "href": "Day2_IRB25_part1.html#reshaping-the-table",
    "title": "Table manipulation",
    "section": "Reshaping the table",
    "text": "Reshaping the table\nYou can create a modified version of an existing data.table by using copy() to avoid changing the original data. Then, use := to add or modify columns by reference.\nIn the example below, we create two new columns (BRCA2 and ATM) filled with random values between 40 and 6000 for 6 rows:\n\ntb.shape <- copy(tb1)\ntb.shape[, \":=\" (\n            BRCA2=sample(40:6000,6),\n            ATM=sample(40:6000,6)) ]\n\ntb.shape\n\n\n\n  \n\n\n\nmelt (wide to long)\nConvert DT to long form where money is a separate observation. measure.vars specify the set of columns we would like to collapse (or combine) together.\n\nmelt(tb.shape, id.vars = c(\"sampleID\", \"cancer\"),\n                measure.vars = c(\"BRCA2\", \"ATM\"))\n\n\n\n  \n\n\n\nWe can also specify column indices instead of names.\n\n(tb1.m1  <- melt(tb.shape, \n                 measure.vars = c(\"BRCA2\", \"ATM\"),\n                 variable.name = \"genes\",\n                 value.name = \"n_mutations\")  \n )\n\n\n\n  \n\n\n\ndcast (long to wide)\nWe want to get the original table from the previous reshaped one using dcast()\n\ndcast(tb1.m1, sampleID + cancer ~ genes, value.var = \"n_mutations\")"
  },
  {
    "objectID": "Day2_IRB25_part1.html#saving-a-table-with-fwrite",
    "href": "Day2_IRB25_part1.html#saving-a-table-with-fwrite",
    "title": "Table manipulation",
    "section": "Saving a table with fwrite()",
    "text": "Saving a table with fwrite()\nThe fwrite() function from the data.table package is a fast and efficient way to write data tables to disk.\n\n\n\n\n\n\nTip\n\n\n\nfwrite(dt, file = “results/iris_summary.csv”)"
  },
  {
    "objectID": "Day1_IRB25_part2.html",
    "href": "Day1_IRB25_part2.html",
    "title": "Functions and conditions",
    "section": "",
    "text": "Functions are reusable blocks of code designed to perform specific tasks. They can take inputs (called arguments), perform operations, and return results. Functions help you write cleaner, more efficient, and more modular code.\nYou can use built-in R functions like mean(), sum(), or length(), or you can create your own custom functions using the function() keyword.\n\n\nIn R, you can define your own functions using the function() keyword.\nA function takes input(s), performs some operations, and usually returns a result which is automatically the last line in the function.\n\n\n\n\n\n\nBasic function structure\n\n\n\n\nfunction_name <- function(argument1, argument2 = default_value) {\n  # Code block that does something with the arguments\n  \n  result <- ...  # Some calculation or value\n  return(result) # Optional: explicitly return the result\n}\n\n\n\n\n\nLet’s look at a simple example of how to write and use a custom function in R.\n\nmy_function <- function(x) {\n  res <- x*2 - 4\n  return(res)\n}\n\nLet’s use the custom function:\n\n# Add two numbers\nmy_function(5)\n\n[1] 6\n\n# Add two vectors\nmy_function(c(1, 2, 3))\n\n[1] -2  0  2\n\n\n\n\n\nA scientist has 48 stem cells. Each one divides into 2 daughter cells.\n\nHow many cells will there be after division?\nHow many full groups of 10 cells can you make?\nHow many cells will be left over?\n\nWe’ll write a custom function that simulates 1 round of cell division to solve this:\n\n# Define a function that takes the number of starting cells and returns division results\ncell_division_summary <- function(initial_cells, \n                                  group_size = 10 # default setting of an argument\n                                  ) {\n  \n  # total number of cells after 1 division\n  total_cells <- initial_cells * 2\n  # how many groups of cells are after 1 division\n  full_groups <- total_cells %/% group_size\n  # how many are leftover\n  leftover_cells <- total_cells %% group_size\n  \n  # Return a list with results since we can only return one line as an result\n  list(\n    total_cells = total_cells,\n    full_groups = full_groups,\n    leftover_cells = leftover_cells\n  )\n}\n\n# Call the function with 48 cells\ncell_division_summary(48)\n\n$total_cells\n[1] 96\n\n$full_groups\n[1] 9\n\n$leftover_cells\n[1] 6\n\n\n\n\nModify the cell_division_summary function so that it supports multiple rounds of cell division.\nEach cycle should double the total number of cells.\nYou should add an additional argument called n_cycles, which:\n\nrepresents the number of division cycles (default should be 1)\nadjusts the total number of cells to be: initial_cells * 2^n_cycles\n\n\n# Write the solution here"
  },
  {
    "objectID": "Day1_IRB25_part1.html",
    "href": "Day1_IRB25_part1.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Hello everyone to your first session “Introduction to R” in the Institute Ruder Boskovic (IRB) workshop. During this session, you will learn the basics to R programming and working environment."
  },
  {
    "objectID": "Day1_IRB25_part1.html#variable",
    "href": "Day1_IRB25_part1.html#variable",
    "title": "Introduction to R",
    "section": "Variable",
    "text": "Variable\n\nCreating a variable\nTo create a certain object, you need to give it a name followed by the assignment operator <- and the value you want to store it. To see the stored value in a variable, you can call it using parentheses or by typing the object name.\n\nvec <- 5\n# Print\n#vec\n# Print\n(vec2 <- 10)\n\n[1] 10\n\n\n\n\nNaming a variable\nWhen naming the variables, they mustn’t be too short or too long. Avoid meaningless variable names and name them logically ( example: mRNA_human ) R is case sensitive ( example: vec is different to Vec ). Some names cannot be used because they are the names of fundamental functions in R (e.g., if, else, for…). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, t, mean, data, df). Furthermore, avoid using dots (.) within a variable name, such as in my.mRNA. A lot of functions in R have dots in their name for historical reasons and dots have special meaning in R so it is best to avoid them."
  },
  {
    "objectID": "Day1_IRB25_part1.html#functions-and-useful-tricks",
    "href": "Day1_IRB25_part1.html#functions-and-useful-tricks",
    "title": "Introduction to R",
    "section": "Functions and useful tricks",
    "text": "Functions and useful tricks\nFunctions are “conserved scripts” that automate more complicated sets of commands including operations assignments, etc. Many functions are predefined, or can be made available by importing R packages (more on that later). A function usually gets one or more inputs called arguments. Functions often (but not always) return a value. Adding ? in front of any function opens a help, description and usage of the function. A typical example function is typeof.\n\n\n\n\n\n\nHelp with functions\n\n\n\nWhat does the function type do?\n(Hint: ?typeof)\n\n\n\n?typeof\n\ntypeof(2)"
  },
  {
    "objectID": "Day1_IRB25_part1.html#vectors",
    "href": "Day1_IRB25_part1.html#vectors",
    "title": "Introduction to R",
    "section": "Vectors",
    "text": "Vectors\nA vector is the most common and basic data type in R. A vector is composed by a series of values. V ectors can be composed of integer/numeric, logical or character values. Using the c()function you can assign a series of values to a vector. The simplest way to create a sequence of numbers in R is by using the : operator.\nExamples of defining vectors\n\n( first_vec_num <- c(1,2,5) )\n\n[1] 1 2 5\n\n#The simplest way to create a sequence of numbers in R is by using the : operator. \n( second_vec_num <- 1:30 )\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n( first_vec_character <- c(\"human\", \"mouse\", \"dog\", \"cat\") )\n\n[1] \"human\" \"mouse\" \"dog\"   \"cat\"  \n\n( first_vec_logic <- c(TRUE, FALSE, TRUE) )\n\n[1]  TRUE FALSE  TRUE\n\n# create a vector containing different types\n( mix_vec <-c(1, \"sea\", TRUE) )\n\n[1] \"1\"    \"sea\"  \"TRUE\"\n\n\n\nDifferent types and classes\n\nTypes\n\ncharacter\ndouble (for double precision floating point numbers)\ninteger\n\n\n\n\n\n\n\nUseful functions for determining types\n\n\n\n\nclass()\nReturns the class of an object (e.g., \"numeric\", \"character\", \"data.frame\"), giving a general idea of its structure.\ntypeof()\nProvides the low-level internal storage type of an object (e.g., \"double\", \"integer\", \"list\"), which can be more specific than class().\nis.numeric(), is.character(), is.logical(), etc.\nReturn TRUE or FALSE depending on whether the object is of the specified type. These are useful for checking expected input types in functions.\nas.numeric(), as.character(), as.logical(), etc.\nCoerce (convert) an object to the specified type, if possible. These are helpful when transforming data types explicitly.\nstr()\nProvides a compact, readable summary of the structure of an R object, including its type, dimensions, and content preview.\n\n\n\nWhat are the types of our objects?\n\n# double/numeric\ntypeof(first_vec_num)\n\n[1] \"double\"\n\n# character\ntypeof(first_vec_character)\n\n[1] \"character\"\n\n# logical\ntypeof(first_vec_logic)\n\n[1] \"logical\"\n\n# character\ntypeof(mix_vec)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\nConverting types from one to another\n\n\n\nNumerical can be converted to character,\n\n# To character\nfirst_vec_num\n\n[1] 1 2 5\n\nas.character(first_vec_num)\n\n[1] \"1\" \"2\" \"5\"\n\n\nwhile not all characters can be converted to numerical:\n\n# To numeric\nfirst_vec_character\n\n[1] \"human\" \"mouse\" \"dog\"   \"cat\"  \n\nas.numeric(first_vec_character)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA NA\n\nas.numeric(mix_vec)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1 NA NA\n\n\n\n\n\n\nCalculation with vectors\nR performs element-wise operations on vectors by default.\nExample: c(1, 2, 3) + c(10, 20, 30) returns 11 22 33\n\nc(1, 2, 3) + c(10, 20, 30)\n\n[1] 11 22 33\n\n\n\n\n\n\n\n\nVectorization\n\n\n\n\nVectorization means operations are applied to all elements at once, without loops.\n\nIt’s efficient and concise.\n\nArithmetic operators (+, -, *, /, ^) are vectorized.\n\nExample: c(2, 4, 6) * 2 returns 4 8 12\n\nFunctions like log(), sqrt(), exp() are also vectorized.\n\nExample: sqrt(c(4, 9, 16)) returns 2 3 4\n\nWhen vectors are of different lengths, R recycles the shorter one (with a warning if not a multiple).\n\nExample: c(1, 2, 3) + 1 returns 2 3 4\n\n\n\nTask\nLet’s go back and see what our variables are holding:\n\n# short vector\nvec\n\n[1] 5\n\n# first vector\nfirst_vec_num\n\n[1] 1 2 5\n\n#second vector\nsecond_vec_num\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n\nWhat will be the outcome of these operations? Explain it with comment in the following code chunk!\n\n# vectorization: all elements in longer first vector were increased by 5\nfirst_vec_num + vec \n\n#\n\n\nfirst_vec_num\n\n[1] 1 2 5\n\n\n\nsecond_vec_num\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n\n\nfirst_vec_num + second_vec_num\n\n\n#\nfirst_vec_num * second_vec_num\n\n\n\n\n\nSubsetting vectors\nSubsetting is a way to access specific elements of a vector using indexing.\nTo extract one or more values, provide the corresponding indices inside square brackets ([]).\n\n\n\n\n\n\nExamples\n\n\n\n\nSelecting the second element\n\n\nfirst_vec_num[2]\n\n[1] 2\n\n\n\nNegative subsetting- meaning not selecting the second element\n\n\nfirst_vec_num[-2]\n\n[1] 1 5\n\n\n\nSelecting first and third elements\n\n\nfirst_vec_num[c(1,3)]\n\n[1] 1 5\n\n\n\n\nMoreover, selection can be done using other vectors, such as logical and numeric.\n\nTask\nWhat happened when you used a logical vector for subsetting?\n\nfirst_vec_num\n\n[1] 1 2 5\n\nfirst_vec_logic\n\n[1]  TRUE FALSE  TRUE\n\n\n\nfirst_vec_num[first_vec_logic]\n\n\nfirst_vec_num\n\n[1] 1 2 5\n\n\n\nfirst_vec_num[first_vec_num]\n\n\n\nAdding and replacing elements\nIn R, you can modify vectors by adding new elements or replacing existing ones using indexing.\n\nTo add elements, simply use the c() function to concatenate new values to the existing vector.\nTo replace elements, use square bracket indexing and assign a new value to a specific position.\n\n\n# Add new values to the end of the vector\nsecond_vec_num <- c(second_vec_num, 1, 30, 30, 30, 1, 1)\nsecond_vec_num\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30  1 30 30 30  1  1\n\n# Replace the first element with a new value\nsecond_vec_num[1] <- 10000\nsecond_vec_num\n\n [1] 10000     2     3     4     5     6     7     8     9    10    11    12\n[13]    13    14    15    16    17    18    19    20    21    22    23    24\n[25]    25    26    27    28    29    30     1    30    30    30     1     1\n\n\n\n\nBoolean Operators\nBoolean operators in R (and other programming languages) are used to perform logical comparisons. They return TRUE or FALSE values based on whether a condition is met.\n\n\n\nOperator\nDescription\nExample\nResult\n\n\n\n\n>\nGreater than\n5 > 6\nFALSE\n\n\n<\nLess than\n5 < 6\nTRUE\n\n\n==\nEquals to\n10 == 10\nTRUE\n\n\n!=\nNot equal to\n10 != 10\nFALSE\n\n\n>=\nGreater than or equal to\n5 >= 6\nFALSE\n\n\n<=\nLess than or equal to\n6 <= 6\nTRUE\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\nBasic inequality: Is 2 not equal to 2?\n\n\n2 != 2\n\n[1] FALSE\n\n\n\nCompare a numeric vector to a single number (element-wise comparison)\n\n\nfirst_vec_num == 2\n\n[1] FALSE  TRUE FALSE\n\n\n\nCompare two vectors of unequal length\n\n\nfirst_vec_num == first_vec_character\n\nWarning in first_vec_num == first_vec_character: longer object length is not a\nmultiple of shorter object length\n\n\n[1] FALSE FALSE FALSE FALSE\n\n\n\n\n\nTask\nIn the second example, we got a warning message longer object length is not a multiple of shorter object length and an output FALSE FALSE FALSE FALSE. What happened here?\n\n# Write your answer here\n\n\n\n\n\nBoolean with logical operators\n\n\n\n\n\n\nThere are three types of logical operators in R\n\n\n\nAND operator &\nOR operator |\nNOT operator !\n\n\nExplain the examples below:\n\nfirst_vec_num\n\n[1] 1 2 5\n\nfirst_vec_num == 5\n\n[1] FALSE FALSE  TRUE\n\nfirst_vec_num[first_vec_num == 5]\n\n[1] 5\n\n###\nfirst_vec_num == 5\n\n[1] FALSE FALSE  TRUE\n\nfirst_vec_num == 2\n\n[1] FALSE  TRUE FALSE\n\nfirst_vec_num == 5 & first_vec_num == 2\n\n[1] FALSE FALSE FALSE\n\n##\nfirst_vec_num == 5 | first_vec_num == 2\n\n[1] FALSE  TRUE  TRUE\n\n\n\n####\nfirst_vec_num[first_vec_num == 5 & first_vec_num == 2]\n\nnumeric(0)\n\nfirst_vec_num[first_vec_num == 5 | first_vec_num == 2]\n\n[1] 2 5\n\n\n\n\nExamples of useful function\n\n\n\n\n\n\nBasic descriptive statistics\n\n\n\n\nsummary()\nProvides a quick overview of an object. For numeric vectors or data frames, it returns the minimum, 1st quartile, median, mean, 3rd quartile, and maximum.\nmean()\nCalculates the arithmetic average of a numeric vector.\nsum()\nAdds all elements of a numeric vector together.\nsd()\nComputes the standard deviation, which measures the amount of variation or dispersion in the data.\ntable()\nCreates a frequency table showing how often each value appears in a vector or factor.\n\n\n\nExample\n\nsum(c(10,10))\n\n[1] 20\n\n# Summary statistics\nsum(second_vec_num)\n\n[1] 10557\n\nsummary(second_vec_num)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    1.00     7.75    16.50   293.25    25.25 10000.00 \n\nsd(second_vec_num)\n\n[1] 1664.043\n\n# Count table\ntable( second_vec_num )\n\nsecond_vec_num\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n    3     1     1     1     1     1     1     1     1     1     1     1     1 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n   27    28    29    30 10000 \n    1     1     1     4     1 \n\ntable( c(\"dog\",\"dog\",\"dog\",\"cat\",\"cat\") )\n\n\ncat dog \n  2   3 \n\n\n\n\n\n\n\n\nVector inspection\n\n\n\n\nlength()\nReturns the number of elements in a vector.\nunique()\nExtracts the distinct (non-duplicate) values from a vector.\nanyNA()\nChecks if there are any missing (NA) values in a vector and returns TRUE or FALSE.\nis.na()\nReturns a logical vector indicating which elements are NA.\nsort()\nArranges the elements of a vector in ascending (default) or descending order.\n\n\n\n\n# Number of elements in vector\nlength(second_vec_num)\n\n[1] 36\n\n# Order a vector\nsort_second_vec <- sort(second_vec_num, \n                          decreasing = FALSE)\nsecond_vec_num; sort_second_vec\n\n [1] 10000     2     3     4     5     6     7     8     9    10    11    12\n[13]    13    14    15    16    17    18    19    20    21    22    23    24\n[25]    25    26    27    28    29    30     1    30    30    30     1     1\n\n\n [1]     1     1     1     2     3     4     5     6     7     8     9    10\n[13]    11    12    13    14    15    16    17    18    19    20    21    22\n[25]    23    24    25    26    27    28    29    30    30    30    30 10000\n\n# Count table\nsort( second_vec_num )\n\n [1]     1     1     1     2     3     4     5     6     7     8     9    10\n[13]    11    12    13    14    15    16    17    18    19    20    21    22\n[25]    23    24    25    26    27    28    29    30    30    30    30 10000\n\n# is there NA\nis.na( second_vec_num ) \n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nanyNA( second_vec_num )\n\n[1] FALSE\n\n\n\nTask\nWhat is the mean of every second element in the vector second_vec_num?\nHard mode: Solve it in two different ways?\n\n# Write the solution here\n\n# first way\nsecond_vec_num\n\n [1] 10000     2     3     4     5     6     7     8     9    10    11    12\n[13]    13    14    15    16    17    18    19    20    21    22    23    24\n[25]    25    26    27    28    29    30     1    30    30    30     1     1\n\n\n\nsum( second_vec_num[c(FALSE, TRUE)] )\n\n[1] 301\n\nsecond_vec_num[c(FALSE, FALSE,TRUE)] \n\n [1]  3  6  9 12 15 18 21 24 27 30 30  1\n\nsecond_vec_num[length(second_vec_num)]\n\n[1] 1"
  },
  {
    "objectID": "Day1_IRB25_part1.html#matrix",
    "href": "Day1_IRB25_part1.html#matrix",
    "title": "Introduction to R",
    "section": "Matrix",
    "text": "Matrix\nMatrix is a two dimensional data structure in R programming. It is similar to vector but additionally contains the dimension attribute.\n\n\n\n\n\n\nDefining a matrix\n\n\n\nYou define a matrix using the matrix function which takes data as vector, number of columns and/or rows to create a matrix. Argument byrow when FALSE is filling the matrix by columns\n\n# filling by rows\nfirst_mat <- matrix(1:20, nrow=5)\nfirst_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n# filling by columns\nsecond_mat <- matrix(1:20, nrow=5,\n                        byrow=TRUE)\nsecond_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n[5,]   17   18   19   20\n\n\n\n\n\nSubsetting a matrix\nSince matrices are two dimensional, you have to subset in both dimensions. Subsetting is done by opening square brackets with supplied index for each dimension separated by a comma. Blank subsetting is useful because you keep all rows or all columns.\n\nfirst_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nfirst_mat[1, 3:4]\n\n[1] 11 16\n\nfirst_mat[1:2, 3:4]\n\n     [,1] [,2]\n[1,]   11   16\n[2,]   12   17\n\n\n\n\nReplacing elements in a matrix\nYou can replace individual elements in a matrix using indexing. Here’s an example:\n\n# Access the element in the first row, first column\nfirst_mat[1,1]\n\n[1] 1\n\n# Replace that element with the character \"a\"\nfirst_mat[1,1] <- \"a\"\n# View the matrix\nfirst_mat\n\n     [,1] [,2] [,3] [,4]\n[1,] \"a\"  \"6\"  \"11\" \"16\"\n[2,] \"2\"  \"7\"  \"12\" \"17\"\n[3,] \"3\"  \"8\"  \"13\" \"18\"\n[4,] \"4\"  \"9\"  \"14\" \"19\"\n[5,] \"5\"  \"10\" \"15\" \"20\"\n\n\n\n\n\n\n\n\nUseful functions for matrices\n\n\n\n\ndim() – Returns or sets the dimensions of a matrix (rows, columns).\nnrow(), ncol() – Get the number of rows or columns.\nt() – Transposes the matrix (swaps rows and columns).\nrowSums(), colSums() – Calculate row-wise or column-wise sums.\nrowMeans(), colMeans() – Calculate means for each row or column.\n\n\n\n\nTask\n\nCreate a matrix called second_matrix using the vector second_vec_num.\nFill the matrix by columns.\nDetermine the dimensions of the matrix.\nCalculate the sum of each row.\nCalculate the sum of each column.\n\n\n# Write the solution here\n\nsecond_matrix <- matrix(second_vec_num, \n                        nrow=5)\n\nWarning in matrix(second_vec_num, nrow = 5): data length [36] is not a\nsub-multiple or multiple of the number of rows [5]\n\nsecond_matrix\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7]  [,8]\n[1,] 10000    6   11   16   21   26    1     1\n[2,]     2    7   12   17   22   27   30 10000\n[3,]     3    8   13   18   23   28   30     2\n[4,]     4    9   14   19   24   29   30     3\n[5,]     5   10   15   20   25   30    1     4\n\n# dimensions\ndim(second_matrix)\n\n[1] 5 8\n\nsecond_matrix\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7]  [,8]\n[1,] 10000    6   11   16   21   26    1     1\n[2,]     2    7   12   17   22   27   30 10000\n[3,]     3    8   13   18   23   28   30     2\n[4,]     4    9   14   19   24   29   30     3\n[5,]     5   10   15   20   25   30    1     4\n\nrowSums(second_matrix)\n\n[1] 10082 10117   125   132   110\n\ncolSums(second_matrix)\n\n[1] 10014    40    65    90   115   140    92 10010"
  },
  {
    "objectID": "Day1_IRB25_part1.html#list",
    "href": "Day1_IRB25_part1.html#list",
    "title": "Introduction to R",
    "section": "List",
    "text": "List\nList is an object containing elements of various types – like strings, numbers, matrices, vectors, functions and another list inside it. The list is created using the list() function in R.\n\nfirst_list <- list(first = c(7,5,24,88),\n                   second = c(\"abc\", 3))\n\nfirst_list\n\n$first\n[1]  7  5 24 88\n\n$second\n[1] \"abc\" \"3\"  \n\n\n\nAccessing elements of a list\nYou can access elements of a list in several ways depending on whether you want:\n\nthe actual value inside the list, or\na sublist containing the element.\n\n\n\n\n\n\n\nDifferent subseting results in different object\n\n\n\n\nIf elements have names, you can access them with $. Here we have names the first stored element as first “first” and therefore we can call it.\n\n\nfirst_list$first\n\n[1]  7  5 24 88\n\n\n\nUsing single brackets [], we get a list\n\n\nfirst_list[1]\n\n$first\n[1]  7  5 24 88\n\n\n\nUsing double brackets [[]], we get a stored element\n\n\nfirst_list[[1]]\n\n[1]  7  5 24 88\n\n\n\nUsing double brackets [[ ]], we access the actual stored element, and then we can subset it further if it’s a vector.\n\n\nfirst_list[[1]][2]\n\n[1] 5\n\n\n\n\n\n\nReplacing and adding elements\nYou can both replace and add values within elements of a list using double brackets [[ ]] and indexing.\n\n# Replace the first value in the first element of the list\nfirst_list[[1]][1] <- \"A\"\n\nAdd a new value to a list element\n\n# Add a new (5th) value to the first element of the list\nfirst_list[[1]][5] <- \"x\" \n\nView the result\n\nfirst_list\n\n$first\n[1] \"A\"  \"5\"  \"24\" \"88\" \"x\" \n\n$second\n[1] \"abc\" \"3\"  \n\n\n\n\nList to vector\nIf you want to convert a list into a single atomic vector, you can use the unlist() function.\nThis flattens all elements of the list into a single vector, as long as the contents are compatible (e.g., numeric and character can both be coerced into character).\n\nunlist(first_list)\n\n first1  first2  first3  first4  first5 second1 second2 \n    \"A\"     \"5\"    \"24\"    \"88\"     \"x\"   \"abc\"     \"3\" \n\nfirst_list\n\n$first\n[1] \"A\"  \"5\"  \"24\" \"88\" \"x\" \n\n$second\n[1] \"abc\" \"3\"  \n\n\n\nTaks\nThe names created when unlisting a list are automatically generated by R and reflect the structure of the original list.\n\nFirst, save the result of unlist into a new vector called unlist_first.\nThen, use the function unname.\nWhat does this do?\n\n\n# Write the solution here\n\nunlist_first <- unlist(first_list)\n#\nunlist_first\n\n first1  first2  first3  first4  first5 second1 second2 \n    \"A\"     \"5\"    \"24\"    \"88\"     \"x\"   \"abc\"     \"3\" \n\nnames(unlist_first)\n\n[1] \"first1\"  \"first2\"  \"first3\"  \"first4\"  \"first5\"  \"second1\" \"second2\"\n\n\n\n# unname\nunlist_first <- unname(unlist_first)"
  },
  {
    "objectID": "Day1_IRB25_part3.html",
    "href": "Day1_IRB25_part3.html",
    "title": "Basic regular expressions",
    "section": "",
    "text": "Regular expressions (regex) are compact patterns used to search, match, and manipulate text in a flexible way.\nA regex is simply a string made up of special characters that define a text pattern."
  },
  {
    "objectID": "Day1_IRB25_part3.html#what-can-regex-represent",
    "href": "Day1_IRB25_part3.html#what-can-regex-represent",
    "title": "Basic regular expressions",
    "section": "What can regex represent?",
    "text": "What can regex represent?\nRegex patterns can describe:\n\nWildcards — match any single character using .\n\nQuantifiers — specify how many times a character or group appears (*, +, {n})\n\nGrouping — combine parts of patterns using parentheses ( )\n\nBoolean OR — match one pattern or another using |\n\nAnchors — specify where in the string to match (^ = start, $ = end)\n\nCharacter Sets — match specific characters using square brackets [abc]"
  },
  {
    "objectID": "Day1_IRB25_part3.html#grep-pattern-matching-with-regex-in-r",
    "href": "Day1_IRB25_part3.html#grep-pattern-matching-with-regex-in-r",
    "title": "Basic regular expressions",
    "section": "grep() – Pattern matching with regex in R",
    "text": "grep() – Pattern matching with regex in R\nThe grep() function is one of the most common ways to use regular expressions in base R.\ng/re/p stands for:\nGlobal search for Regular Expression and Print matching lines\n\n\n\n\n\n\nBasic grep syntax\n\n\n\n\ngrep(\"pattern\", your_vector)\n\n\nBy default, it returns the positions of matching elements.\nTo return the matching values, use: value = TRUE\nTo return the indices or values that do not match, use: invert = TRUE\n\n\n\n\n\n\n\n\n\nExample : How to use grep?\nLet’s define a new character vector with a mix of names, codes, and text snippets:\n\nmy_strings <- c(\n  \"apple123\", \"banana\", \"cat!\", \"dog99\", \"zebra_2\", \n  \"Xray\", \"alpha-beta\", \"beta.alpha\", \"code42\", \"hello world!\",\n  \"abAB12\", \"123abc\", \"abc!\", \"end.\", \"Begin!\", \"test_case?\",\n  \"no_numbers\", \"DATA2025\", \"regex_fun\", \"start_end\"\n)\n\nFind strings that contain numbers\n\ngrep(\"\\\\d\", my_strings, value = TRUE)\n\n[1] \"apple123\" \"dog99\"    \"zebra_2\"  \"code42\"   \"abAB12\"   \"123abc\"   \"DATA2025\"\n\n\nFind strings that contain underscores\n\ngrep(\"_\", my_strings, value = TRUE)\n\n[1] \"zebra_2\"    \"test_case?\" \"no_numbers\" \"regex_fun\"  \"start_end\" \n\n\nFind strings that start with a lowercase letter\n\ngrep(\"^[a-z]\", my_strings, value = TRUE)\n\n [1] \"apple123\"     \"banana\"       \"cat!\"         \"dog99\"        \"zebra_2\"     \n [6] \"alpha-beta\"   \"beta.alpha\"   \"code42\"       \"hello world!\" \"abAB12\"      \n[11] \"abc!\"         \"end.\"         \"test_case?\"   \"no_numbers\"   \"regex_fun\"   \n[16] \"start_end\"   \n\n\n\nHow to find special characters?\nSometimes you want to match characters that are “special” in regex, like ., *, ?, (, ), etc.\nTo do this, you must escape them using a double backslash (\\\\) in R.\nLet’s say we want to find strings that contain a question mark:\n\ngrep(\"\\\\?\", my_strings, value = TRUE)\n\n[1] \"test_case?\"\n\n\n\nTasks\n\nFind strings that end with number\n\n\n# Write the solution here\n\n\nFind strings that do not start with capital letters\n\n\n# Write the solution here"
  },
  {
    "objectID": "Day2_IRB25_part0.html",
    "href": "Day2_IRB25_part0.html",
    "title": "R packages",
    "section": "",
    "text": "CRAN is the official repository for R packages. It hosts thousands of stable, peer-reviewed R packages that have passed strict checks for quality and compatibility.\n\n#Install from CRAN as follow:\ninstall.packages(\"magrittr\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\") \n\n\n\n\nGitHub is a platform for code sharing and collaboration, widely used by developers. Many R package authors share their development versions on GitHub—these might include the newest features or bug fixes that haven’t yet been submitted to CRAN.\n\n#Or, install the latest version from GitHub as follow:\n# Install\nif(!require(devtools)) install.packages(\"devtools\")\ndevtools::install_github(\"kassambara/ggpubr\")\n\n\n\n\nBioconductor is a specialized repository for packages focused on bioinformatics, genomics, and computational biology. It includes tools for analyzing DNA/RNA sequencing, gene expression, and other biological data.\n\n## Bioconductor\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"airway\")"
  },
  {
    "objectID": "Day2_IRB25_part0.html#load-the-packages",
    "href": "Day2_IRB25_part0.html#load-the-packages",
    "title": "R packages",
    "section": "Load the packages",
    "text": "Load the packages\nWhen you install a package in R (using install.packages() or similar), you’re downloading it and saving it to your computer. This only needs to be done once (per R environment).\nHowever, to use the functions or data from a package in your current R session, you must load the package with library() or require().\n\nlibrary(\"magrittr\")\nlibrary(\"data.table\")\nlibrary(\"ggpubr\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "IRB workshop 2025",
    "section": "",
    "text": "👋 Welcome to the IRB Workshop 2025 Website\nThis website hosts the material for the IRB Workshop 2025. You’ll find R code examples, tasks, and explanations for each session."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IRB workshop 2025",
    "section": "",
    "text": "👋 Welcome to the IRB Workshop 2025 Website\nThis website hosts the material for the IRB Workshop 2025. You’ll find R code examples, tasks, and explanations for each session."
  },
  {
    "objectID": "Day3_IRB25_part0.html",
    "href": "Day3_IRB25_part0.html",
    "title": "Practice data.table and ggplot2",
    "section": "",
    "text": "In the first code chunk, load the data.table and ggplot2 libraries — these will be essential for data manipulation and visualization throughout the analysis.\n\n# Import the libraries\nlibrary(data.table)\nlibrary(ggplot2)"
  },
  {
    "objectID": "Introduction_to_stats.html",
    "href": "Introduction_to_stats.html",
    "title": "Introduction to statistics",
    "section": "",
    "text": "We will investigate another build-in dataset mtcars.\nLet’s say that our research question will be: “Which factors are most associated with fuel efficiency (mpg) in cars?”\nWe will treat mpg (miles per gallon) as the outcome variable and analyze the effects of other available characteristics on it.\n\n\nWhere can we obtain some information about this dataset?\n\n\n\n\n\n\nWhat is the very first thing you do when starting a new analysis?\n\n# Data manipulation\nlibrary(data.table)\n\n# Visualization\nlibrary(ggplot2)\nlibrary(ggpubr)        # For gghistogram, ggboxplot, ggscatter, stat_compare_means, stat_cor\nlibrary(corrplot)      # For correlation matrix plot\n\n# Outlier detection\nlibrary(rstatix)       # identify_outliers\nlibrary(outliers)      # grubbs.test\nlibrary(EnvStats)      # rosnerTest\n\n# Correlation matrix with p-values\nlibrary(Hmisc)         # rcorr\n\n# Variance Inflation Factor (VIF) to check multicollinearity\nlibrary(car)          # vif\n\n\ndt <- as.data.table(mtcars)\nstr(dt)\n\nClasses 'data.table' and 'data.frame':  32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\nsummary(dt)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\nhead(dt)\n\n\n\n  \n\n\n\nWhat can you conclude from your data?\n\nmpg: Fuel efficiency (miles per gallon) - dependent variable\nhp, wt, disp: Numeric predictors\nam, cyl, gear: Categorical factors\n\n\n\n\n\nlapply(dt[, .(cyl, am, gear)], table)\n\n$cyl\n\n 4  6  8 \n11  7 14 \n\n$am\n\n 0  1 \n19 13 \n\n$gear\n\n 3  4  5 \n15 12  5 \n\n\n\n\n\nWe want to do this to ensure that R will treat am, gear, cyl as categorical variables, not continuous numerical. This is crucial because comparing means across groups (e.g. Automatic vs Manual) requires factors, not numbers.\n\n# Apply factor conversion with correct labeling\n# 'am' is 0 = Automatic, 1 = Manual\ndt[, cyl := as.factor(cyl)]\ndt[, gear := as.factor(gear)]\ndt[, am := factor(am, levels = c(0, 1), labels = c(\"Automatic\", \"Manual\"))]\n\nCheck categorical variables (again)\n\nlapply(dt[, .(cyl, am, gear)], table)\n\n$cyl\n\n 4  6  8 \n11  7 14 \n\n$am\n\nAutomatic    Manual \n       19        13 \n\n$gear\n\n 3  4  5 \n15 12  5 \n\n\n\n\nCan you define what type of data is factor in R?\n\n\n\nMake descriptive statistics for mpg variable grouped by 1) am and 2) cyl.\n\n# Write your solution here\n\n\n\n\n\n\nTo evaluate some descriptive statistics we can first visually inspect the data.\n\n\n\n# Make histograms\ngghistogram(dt, x = \"mpg\", fill = \"cyl\")\n\n\n\ngghistogram(dt, x = \"mpg\", fill = \"am\")\n\n\n\ngghistogram(dt, x = \"mpg\", fill = \"cyl\", facet.by = \"am\")\n\n\n\n\n\n\nWhat function ggdensity does?\n\n# Make Q-Q plot\nggqqplot(dt, x = \"mpg\")\n\n\n\nggqqplot(dt, x = \"mpg\", color = \"cyl\", facet.by = \"cyl\")\n\n\n\nggqqplot(dt, x = \"mpg\", color = \"am\", facet.by = \"am\")\n\n\n\n\n\n# Run statistical test\nshapiro.test(dt$mpg)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt$mpg\nW = 0.94756, p-value = 0.1229\n\n\n\n\n\n\n\nggboxplot(dt, x = \"am\", y = \"mpg\", fill = \"am\") +\n  stat_compare_means(method = \"t.test\")\n\n\n\n\n\n\nCan you detect any outliers in other variables?\n\n# Write your solution here\n\n\n\n\n\nOutlier detection using descriptive statistics methods\nApart from visual inspection with boxplots, outliers can also be detected via several descriptive statistics (including minimum, maximum, histogram, and percentiles) or thanks to more formal techniques of outliers detection using statistical tests. Once you detect a possible outlier, it is up to you to decide how to treat them (i.e., keeping, removing or imputing them) before conducting further analyses.\nOutlier detection using descriptive statistics methods\n\n# SD method - those that are 2 x SD away from mean\nmean_mpg <- mean(dt$mpg)\nsd_mpg <- sd(dt$mpg)\ndt[mpg < mean_mpg - 2 * sd_mpg | mpg > mean_mpg + 2 * sd_mpg]\n\n\n\n  \n\n\n\n\n# IQR method - those that are 1.5 x IQR away from 1st or 3rd quartile\nidentify_outliers(data = dt, variable = \"mpg\")\n\n\n\n  \n\n\n\n\n# Z-score method (if your data has a normal distribution)\n# A z-score below -3.29 or above 3.29 means you have detected outliers. \n# This value of 3.29 comes from the fact that 1 observation out of 1000 is out of this interval if the data follow a normal distribution.\nz_scores <- scale(dt$mpg)\ndt[, z_mpg := z_scores]\ndt[abs(z_mpg) > 3.29]\n\n\n\n  \n\n\n\n\n\nCan you conclude what scale does?\n\n\n\n\nTake into consideration that these tests assume normality of the data.\nThe Grubbs test\nIt allows to detect whether the highest or lowest value in a dataset is an outlier. It only detects one outlier at a time. It is not appropriate for sample size of 6 or less.\n\ngrubbs.test(dt$mpg)\n\n\n    Grubbs test for one outlier\n\ndata:  dt$mpg\nG = 2.29127, U = 0.82518, p-value = 0.276\nalternative hypothesis: highest value 33.9 is an outlier\n\ngrubbs.test(dt$mpg, opposite = TRUE)\n\n\n    Grubbs test for one outlier\n\ndata:  dt$mpg\nG = 1.60788, U = 0.91391, p-value = 1\nalternative hypothesis: lowest value 10.4 is an outlier\n\n\nDixon test\nSimilar to the Grubbs test, Dixon test is used to test whether a single low or high value is an outlier. So if more than one outliers is suspected, the test has to be performed on these suspected outliers individually. This test is most useful for small sample size (usually n ≤ 25).\n\ndixon.test(dt$mpg)\n\nError in dixon.test(dt$mpg): Sample size must be in range 3-30\n\n\nIt is a good practice to always check the results of the statistical test for outliers against the boxplot to make sure we tested all potential outliers.\nIf outlier is detected, find and exclude highest value (i.e. outlier) and then repeat the Dixon test on dataset without this outlier.\nRosner test\nIt is used to detect several outliers at once (unlike Grubbs and Dixon test which must be performed iteratively to screen for multiple outliers), and it is designed to avoid the problem of masking, where an outlier that is close in value to another outlier can go undetected. It is most appropriate when the sample size is large (n ≥ 20).\n\nrosnerTest(dt$mpg, k = 1)\n\n$distribution\n[1] \"Normal\"\n\n$statistic\n     R.1 \n2.291272 \n\n$sample.size\n[1] 32\n\n$parameters\nk \n1 \n\n$alpha\n[1] 0.05\n\n$crit.value\nlambda.1 \n2.938048 \n\n$n.outliers\n[1] 0\n\n$alternative\n[1] \"Up to 1 observations are not\\n                                 from the same Distribution.\"\n\n$method\n[1] \"Rosner's Test for Outliers\"\n\n$data\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n$data.name\n[1] \"dt$mpg\"\n\n$bad.obs\n[1] 0\n\n$all.stats\n  i   Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier\n1 0 20.09062 6.026948  33.9      20 2.291272   2.938048   FALSE\n\nattr(,\"class\")\n[1] \"gofOutlier\"\n\n\n\n\n\nWe can first visually inspect correlations by creating scatter plots:\n\nggscatter(data = dt, \n          x = \"am\", \n          y = \"mpg\", \n          add = \"reg.line\")\n\n\n\n\nWhat if you want to do correlation analysis for all available numeric variables?\n\ncor_data <- dt[, .(mpg, hp, wt, qsec)]\ncor_matrix <- cor(cor_data)\ncorrplot(cor_matrix, method = 'square', type = 'upper', diag = FALSE, tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\nCorrelation ≠ Collinearity\n\n\n\nCorrelation shows linear association between two variables only.\nMulticollinearity can involve more than two variables, and this isn’t captured by correlation alone.\nTwo variables can have low pairwise correlation, but still cause multicollinearity in combination with others."
  },
  {
    "objectID": "Introduction_to_ML.html",
    "href": "Introduction_to_ML.html",
    "title": "Introduction to machine learning",
    "section": "",
    "text": "# Data manipulation\nlibrary(data.table)\n\n# Visualization\nlibrary(ggplot2)\nlibrary(ggpubr)        \nlibrary(corrplot)  \n\n# Machine learning\nlibrary(caret)\nlibrary(MASS)\nlibrary(factoextra)\nlibrary(Boruta)\n\nset.seed() in R sets the random number generator seed, which ensures that any random operations (like splitting data into training/test sets, random sampling, cross-validation folds) are reproducible.\nWhen you perform these tasks they involve randomness. Without set.seed(), the results would change every time you run the code. By setting a seed, you ensure that the same random split happens every time, and that others can reproduce your results exactly.\n\n# Set seed for reproducibility\nset.seed(12345)"
  },
  {
    "objectID": "Introduction_to_ML.html#convert-numerical-variables-to-categorical",
    "href": "Introduction_to_ML.html#convert-numerical-variables-to-categorical",
    "title": "Introduction to statistics",
    "section": "Convert numerical variables to categorical",
    "text": "Convert numerical variables to categorical\n\nheart_disease[, Sex := as.factor(Sex)]\nheart_disease[, ChestPain := as.factor(ChestPain)]\nheart_disease[, BloodSugar := as.factor(BloodSugar)]\nheart_disease[, ExerciseInducedAngina := as.factor(ExerciseInducedAngina)]\nheart_disease[, HeartDisease := as.factor(HeartDisease)]\n\nstr(heart_disease)\n\nClasses 'data.table' and 'data.frame':  303 obs. of  10 variables:\n $ rownames             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Age                  : int  63 67 67 37 41 56 62 57 63 53 ...\n $ Sex                  : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 1 1 2 2 ...\n $ ChestPain            : Factor w/ 4 levels \"Asymptomatic\",..: 4 1 1 3 2 2 1 1 1 1 ...\n $ BP                   : int  145 160 120 130 130 120 140 120 130 140 ...\n $ Cholesterol          : int  233 286 229 250 204 236 268 354 254 203 ...\n $ BloodSugar           : Factor w/ 2 levels \"FALSE\",\"TRUE\": 2 1 1 1 1 1 1 1 1 2 ...\n $ MaximumHR            : int  150 108 129 187 172 178 160 163 147 155 ...\n $ ExerciseInducedAngina: Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 2 1 2 ...\n $ HeartDisease         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 2 1 2 2 ...\n - attr(*, \".internal.selfref\")=<externalptr>"
  },
  {
    "objectID": "Introduction_to_ML.html#evaluate-correlation-patterns",
    "href": "Introduction_to_ML.html#evaluate-correlation-patterns",
    "title": "Introduction to machine learning",
    "section": "Evaluate correlation patterns",
    "text": "Evaluate correlation patterns\n\n# Calculate Pearson's correlation coefficient\ncor_matrix <- cor(heart_disease[, .SD, .SDcols = is.numeric][, -1],\n                  method = \"pearson\", \n                  use = \"complete.obs\")\n\n# Plot corrplot to evaluate correlation patterns among the data\ncorrplot(cor_matrix,\n         method = \"color\",      \n         type = \"upper\", \n         diag = FALSE,\n         order = \"hclust\",      \n         tl.cex = 0.7,    \n         tl.col = \"black\", addCoef.col = \"black\",\n         col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200))"
  },
  {
    "objectID": "Introduction_to_ML.html#data-scaling",
    "href": "Introduction_to_ML.html#data-scaling",
    "title": "Introduction to machine learning",
    "section": "Data scaling",
    "text": "Data scaling\nBefore fitting any machine learning model, scaling of the dataset needs to be performed. Usually, all algorithms are sensitive to variables that have incomparable units, leading to misleading results.\nTo avoid this problem (because all variables should be treated equally), the variables need to be transformed to be on a similar scale which allows them to be compared correctly using the distance metric.\nThe most popular method for that is standardization, which consists in subtracting the average value from the feature value and then dividing it by its standard deviation. This technique will allow obtaining features with a mean of 0 and a deviation of 1.\nStandardization is often used as a pre-processing step in machine learning to make sure that all the variables have the same scale and same importance in the model. Standardization is not sensitive to outliers.\nIn R, standardization can be done with the scale() function.\n\n# Summary of the data\nsummary(heart_disease)\n\n    rownames          Age            Sex                 ChestPain  \n Min.   :  1.0   Min.   :29.00   Female: 97   Asymptomatic    :144  \n 1st Qu.: 76.5   1st Qu.:48.00   Male  :206   Atypical angina : 50  \n Median :152.0   Median :56.00                Non-anginal pain: 86  \n Mean   :152.0   Mean   :54.44                Typical angina  : 23  \n 3rd Qu.:227.5   3rd Qu.:61.00                                      \n Max.   :303.0   Max.   :77.00                                      \n       BP         Cholesterol    BloodSugar    MaximumHR    \n Min.   : 94.0   Min.   :126.0   FALSE:258   Min.   : 71.0  \n 1st Qu.:120.0   1st Qu.:211.0   TRUE : 45   1st Qu.:133.5  \n Median :130.0   Median :241.0               Median :153.0  \n Mean   :131.7   Mean   :246.7               Mean   :149.6  \n 3rd Qu.:140.0   3rd Qu.:275.0               3rd Qu.:166.0  \n Max.   :200.0   Max.   :564.0               Max.   :202.0  \n ExerciseInducedAngina HeartDisease\n No :204               No :164     \n Yes: 99               Yes:139     \n                                   \n                                   \n                                   \n                                   \n\n# Data scaling\nheart_disease_scaled <- scale(heart_disease[, .SD, .SDcols = is.numeric][, -1])\n\n# Summary of the normalized data\nsummary(heart_disease_scaled)\n\n      Age                BP            Cholesterol        MaximumHR      \n Min.   :-2.8145   Min.   :-2.14149   Min.   :-2.3310   Min.   :-3.4364  \n 1st Qu.:-0.7124   1st Qu.:-0.66420   1st Qu.:-0.6894   1st Qu.:-0.7041  \n Median : 0.1727   Median :-0.09601   Median :-0.1100   Median : 0.1483  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.7259   3rd Qu.: 0.47218   3rd Qu.: 0.5467   3rd Qu.: 0.7166  \n Max.   : 2.4961   Max.   : 3.88132   Max.   : 6.1283   Max.   : 2.2904"
  },
  {
    "objectID": "Introduction_to_ML.html#pca-analysis",
    "href": "Introduction_to_ML.html#pca-analysis",
    "title": "Introduction to machine learning",
    "section": "PCA analysis",
    "text": "PCA analysis\n\n# Run PCA\npca_result <- prcomp(heart_disease_scaled, scale. = FALSE)\n\n# Combine PCA and original label\npca_df <- data.frame(pca_result$x, \n                     diagnosis = heart_disease$HeartDisease)\n\n# Plot PC1 vs PC2 and color by diagnosis\nggplot(pca_df, aes(PC1, PC2, col = diagnosis)) +\n  geom_point(size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"\", color = \"Diagnosis\")\n\n\n\n# Explained variance\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4\nStandard deviation     1.2580 1.0208 0.9294 0.7154\nProportion of Variance 0.3956 0.2605 0.2159 0.1280\nCumulative Proportion  0.3956 0.6561 0.8720 1.0000"
  },
  {
    "objectID": "Introduction_to_ML.html#k-means-clustering",
    "href": "Introduction_to_ML.html#k-means-clustering",
    "title": "Introduction to machine learning",
    "section": "K-means clustering",
    "text": "K-means clustering\n\nkm.out_initial <- kmeans(heart_disease_scaled, \n                         centers = 2,\n                         nstart = 20)\nkm.out_initial\n\nK-means clustering with 2 clusters of sizes 154, 149\n\nCluster means:\n         Age         BP Cholesterol  MaximumHR\n1  0.6993066  0.3913779   0.2368136 -0.5619240\n2 -0.7227733 -0.4045114  -0.2447603  0.5807805\n\nClustering vector:\n  [1] 1 1 1 2 2 2 1 1 1 2 1 1 1 2 1 2 2 2 2 2 1 1 2 2 1 2 2 1 2 2 1 2 1 1 2 2 2\n [38] 1 1 1 1 2 1 1 1 2 2 1 1 2 2 1 2 2 1 1 2 2 2 1 1 2 1 2 1 1 1 2 1 2 1 1 1 1\n [75] 2 1 1 1 2 1 2 1 2 1 2 2 2 1 2 2 1 1 1 2 1 2 1 1 2 2 2 2 1 1 2 2 2 1 1 2 1\n[112] 1 2 2 1 2 2 2 1 1 2 1 2 1 1 2 1 1 2 2 2 2 2 2 2 2 1 1 2 2 1 1 2 1 2 2 1 2\n[149] 2 2 2 2 1 1 1 1 2 2 1 1 2 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 2 1 2 2 2 1 2 1 1\n[186] 2 2 1 1 1 2 1 2 1 1 1 1 2 2 1 2 1 2 1 2 2 1 1 2 1 2 2 2 1 2 2 2 2 1 2 2 2\n[223] 2 1 2 2 2 1 1 1 2 1 2 1 1 1 1 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 2 2 2 1 1 1\n[260] 1 2 1 1 2 1 1 2 1 2 2 1 1 1 1 2 1 1 2 1 1 1 2 1 2 1 1 1 1 2 2 1 1 2 1 1 2\n[297] 1 1 2 1 1 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 554.5513 335.8822\n (between_SS / total_SS =  26.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# Evaluate size of each cluster\nkm.out_initial$size\n\n[1] 154 149\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHow to select for optimal number of clusters?\nWhen you run k-means clustering with different values of k (e.g., from 1 to 10), the algorithm calculates a value called Total Within-Cluster Sum of Squares (WSS) - it measures how compact the clusters are (i.e. how close data points are to their assigned cluster centers).\nAs k increases, WSS decreases because more clusters mean smaller, tighter groups. But adding more clusters always reduces WSS and at some point, the benefit becomes marginal.\nFor visual representation of this we can run Elbow plot and find a point where the rate of decrease sharply changes - that elbow suggests an optimal balance.\n\n\n\n# Elbow method \nfviz_nbclust(heart_disease_scaled, \n             kmeans, \n             method = \"wss\")\n\n\n\n\nLet’s visualize our K-means clusters:\n\n# Combine K-means and original label\nkmeans_df <- cbind(heart_disease, \n                   cluster = as.factor(km.out_initial$cluster)\n                   )\n\n# How well did k-means clustering do in comparison with known groups?\ntable(kmeans_df$cluster, kmeans_df$HeartDisease)\n\n   \n     No Yes\n  1  56  98\n  2 108  41\n\n\nLet’s merge now our K-means clustering with PCA visualization\n\n# Visualize PCA with cluster assignment\nfviz_pca_ind(prcomp(heart_disease_scaled), \n             geom.ind = \"point\", \n             col.ind = as.factor(kmeans_df$cluster), \n             palette = \"jco\",\n             addEllipses = TRUE,\n             legend.title = \"Cluster\")"
  },
  {
    "objectID": "Introduction_to_ML.html#hierarchical-clustering",
    "href": "Introduction_to_ML.html#hierarchical-clustering",
    "title": "Introduction to machine learning",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nThis is an alternative to k-means clustering, with big advantage that it does not require pre-selection of the number of clusters to obtain in the end.\n\n# Calculate clusters based on default distance metrics - evaluate others!\nhclust_result <- hclust(dist(heart_disease_scaled), \n                        method = \"complete\")\n\n# Create dendogram\nplot(hclust_result)\n\n\n\n\nFor improved visualizations please examine here (strongly recommended):"
  },
  {
    "objectID": "Introduction_to_ML.html#feature-selection",
    "href": "Introduction_to_ML.html#feature-selection",
    "title": "Introduction to machine learning",
    "section": "Feature selection",
    "text": "Feature selection\nWhen we have huge number of predictors we want to first reduce the number of features. With that we can: Improve model performance - irrelevant or redundant variables can confuse the model, causing overfitting. Reduce overfitting - models trained on too many features may fit noise instead of signal. Speed up training - important for complex models or large datasets. Improve model interpretability - fewer variables make it easier to understand and explain the model’s decisions. Avoid multicollinearity - helps remove such redundant variables.\n\nFeature selection with stepwise regression model\n\n# Full model\nfull_model <- glm(HeartDisease ~ ., \n                  data = heart_disease, \n                  family = binomial)\n\n# Perform backward selection & select variables\nstep_model_back <- stepAIC(full_model, \n                           direction = \"backward\", \n                           trace = FALSE)\nselected_vars_back <- names(coef(step_model_back))[-1]\nselected_vars_back\n\n[1] \"SexMale\"                   \"ChestPainAtypical angina\" \n[3] \"ChestPainNon-anginal pain\" \"ChestPainTypical angina\"  \n[5] \"BP\"                        \"Cholesterol\"              \n[7] \"MaximumHR\"                 \"ExerciseInducedAnginaYes\" \n\n# Perform forward selection & select variables\nstep_model_for <- stepAIC(full_model, \n                          direction = \"forward\",\n                          trace = FALSE)\nselected_vars_for <- names(coef(step_model_for))[-1]\nselected_vars_for\n\n [1] \"rownames\"                  \"Age\"                      \n [3] \"SexMale\"                   \"ChestPainAtypical angina\" \n [5] \"ChestPainNon-anginal pain\" \"ChestPainTypical angina\"  \n [7] \"BP\"                        \"Cholesterol\"              \n [9] \"BloodSugarTRUE\"            \"MaximumHR\"                \n[11] \"ExerciseInducedAnginaYes\" \n\n# Perform both selection & select variables\nstep_model_both <- stepAIC(full_model,\n                           direction = \"both\", \n                           trace = FALSE)\nselected_vars_both <- names(coef(step_model_both))[-1]\nselected_vars_both\n\n[1] \"SexMale\"                   \"ChestPainAtypical angina\" \n[3] \"ChestPainNon-anginal pain\" \"ChestPainTypical angina\"  \n[5] \"BP\"                        \"Cholesterol\"              \n[7] \"MaximumHR\"                 \"ExerciseInducedAnginaYes\" \n\n\n\n\nFeature selection with Boruta random forest model\nBoruta is a feature selection algorithm that adds shadow features (randomized copies of original features), trains a Random Forest model, compares importance of original vs shadow features, and iteratively confirms, rejects, or leaves undecided the features.\nIt gives a robust, data-driven way to filter out noise and works well with non-linear relationships.\nUnlike stepwise methods, it respects feature interactions.\n\n# Run Boruta\nmodel_boruta <- Boruta(HeartDisease ~ ., \n                       data = heart_disease)\nprint(model_boruta)\n\nBoruta performed 32 iterations in 1.838308 secs.\n 6 attributes confirmed important: Age, BP, ChestPain,\nExerciseInducedAngina, MaximumHR and 1 more;\n 3 attributes confirmed unimportant: BloodSugar, Cholesterol, rownames;\n\nplot(model_boruta)\n\n\n\n# Select confirmed features\nselected_var <- getSelectedAttributes(model_boruta)\n\n# Resolve tentative features (if any)\nfinal_boruta <- TentativeRoughFix(model_boruta)\ngetSelectedAttributes(final_boruta)\n\n[1] \"Age\"                   \"Sex\"                   \"ChestPain\"            \n[4] \"BP\"                    \"MaximumHR\"             \"ExerciseInducedAngina\""
  },
  {
    "objectID": "Introduction_to_ML.html#splitting-data-into-training-and-testing-sets",
    "href": "Introduction_to_ML.html#splitting-data-into-training-and-testing-sets",
    "title": "Introduction to machine learning",
    "section": "Splitting data into training and testing sets",
    "text": "Splitting data into training and testing sets\nNow we’ll use the caret library to split the dataset into training and testing sets. The createDataPartition function from the caret library helps us achieve this. We’ll partition 70% of the data for training and the remaining 30% for testing.\nBy splitting the data, you ensure that your model is trained and tested on different data, which helps in evaluating its real-world performance.\n\nheart_disease <- as.data.frame(heart_disease[, -1])\n\n# Splitting data into training and testing sets\ntrainIndex <- createDataPartition(heart_disease$HeartDisease, \n                                  p = 0.7, \n                                  list = FALSE, \n                                  times = 1)\ntrainData <- heart_disease[trainIndex, ]\ntestData <- heart_disease[-trainIndex, ]\n\n# Print the number of rows in training and testing sets\nprint(nrow(trainData))\n\n[1] 213\n\ntable(trainData$HeartDisease)\n\n\n No Yes \n115  98 \n\nprint(nrow(testData))\n\n[1] 90\n\ntable(testData$HeartDisease)\n\n\n No Yes \n 49  41"
  },
  {
    "objectID": "Introduction_to_ML.html#feature-scaling",
    "href": "Introduction_to_ML.html#feature-scaling",
    "title": "Introduction to machine learning",
    "section": "Feature scaling",
    "text": "Feature scaling\nFeature scaling is an important step to ensure that all data points are on a similar scale. This is especially important for algorithms that use distance measurements (e.g., K-Nearest Neighbors).\nWe’ll normalize (center and scale) the features using the preProcess function from the caret library. preProcess computes scaling parameters predict applies scaling to the data\n\n# Feature scaling (excluding factor columns)\nnumericColumns <- sapply(trainData, is.numeric)\npreProcValues <- preProcess(trainData[, numericColumns[-1]], method = c(\"center\", \"scale\"))\ntrainData[, numericColumns] <- predict(preProcValues, \n                                       trainData[, numericColumns])\ntestData[, numericColumns] <- predict(preProcValues, \n                                      testData[, numericColumns])"
  },
  {
    "objectID": "Introduction_to_ML.html#building-and-evaluating-models",
    "href": "Introduction_to_ML.html#building-and-evaluating-models",
    "title": "Introduction to machine learning",
    "section": "Building and evaluating models",
    "text": "Building and evaluating models\n\nTrain a logistic regression model\n\nglm_model <- train(HeartDisease ~ ., \n                   data = trainData, \n                   method = \"glm\", \n                   family = \"binomial\")\nglm_model\n\nGeneralized Linear Model \n\n213 samples\n  8 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 213, 213, 213, 213, 213, 213, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7467924  0.4877528\n\nsummary(glm_model)\n\n\nCall:\nNULL\n\nCoefficients:\n                             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  2.236059   2.614243   0.855 0.392365    \nAge                          0.017221   0.025332   0.680 0.496629    \nSexMale                      1.907153   0.473389   4.029 5.61e-05 ***\n`ChestPainAtypical angina`  -1.831258   0.551094  -3.323 0.000891 ***\n`ChestPainNon-anginal pain` -1.867625   0.464303  -4.022 5.76e-05 ***\n`ChestPainTypical angina`   -2.323870   0.715857  -3.246 0.001169 ** \nBP                           0.483273   0.200360   2.412 0.015864 *  \nCholesterol                  0.005616   0.003792   1.481 0.138597    \nBloodSugarTRUE              -0.134908   0.482257  -0.280 0.779675    \nMaximumHR                   -0.035188   0.011113  -3.166 0.001543 ** \nExerciseInducedAnginaYes     0.584593   0.435873   1.341 0.179855    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 293.92  on 212  degrees of freedom\nResidual deviance: 189.63  on 202  degrees of freedom\nAIC: 211.63\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nAccuracy: This measures the proportion of correct predictions made by the model out of all predictions. For example, an accuracy of 0.76 means the model correctly predicted 77% of the cases.\nKappa: This adjusts the accuracy to account for the possibility of the agreement occurring by chance. A Kappa value of 1 indicates perfect agreement, while 0 means the agreement is no better than random guessing.\n\n\n\nMake predictions and evaluate the model\n\n# Making predictions\npredictions <- predict(glm_model, testData)\n\n# Evaluating the model\nconfusion <- confusionMatrix(predictions, testData$HeartDisease)\nprint(confusion)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  44  10\n       Yes  5  31\n                                        \n               Accuracy : 0.8333        \n                 95% CI : (0.74, 0.9036)\n    No Information Rate : 0.5444        \n    P-Value [Acc > NIR] : 7.067e-09     \n                                        \n                  Kappa : 0.6606        \n                                        \n Mcnemar's Test P-Value : 0.3017        \n                                        \n            Sensitivity : 0.8980        \n            Specificity : 0.7561        \n         Pos Pred Value : 0.8148        \n         Neg Pred Value : 0.8611        \n             Prevalence : 0.5444        \n         Detection Rate : 0.4889        \n   Detection Prevalence : 0.6000        \n      Balanced Accuracy : 0.8270        \n                                        \n       'Positive' Class : No            \n                                        \n\n\nA confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\n\nTrue Positives (TP): The number of correct positive predictions.\nTrue Negatives (TN): The number of correct negative predictions.\nFalse Positives (FP): The number of incorrect positive predictions (also known as Type I errors).\nFalse Negatives (FN): The number of incorrect negative predictions (also known as Type II errors).\n\n\n\nVisual inspection of model and the importance of different features\n\n# Visualizing the logistic regression coefficients\ncoef_df <- as.data.frame(coef(summary(glm_model$finalModel)))\ncoef_df$Variable <- rownames(coef_df)\nnames(coef_df)[1] <- \"Estimate\"\n\nscatter_plot <- ggplot(coef_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +\n  geom_bar(stat = 'identity') +\n  coord_flip() +\n  theme_light() +\n  labs(title = \"Variable Importance (Logistic Regression)\",\n       x = \"Variables\",\n       y = \"Estimate\")\n\n\n\nModel generalization and validation through cross-validation\nCross-validation is a technique used to evaluate how well your model generalizes to unseen data by splitting the dataset into multiple subsets or “folds”. The model is trained on a portion of the data and validated on the remaining part, rotating through the folds to get a comprehensive performance metric. This helps ensure that your model isn’t just fitting noise in your training data but can perform well on independent datasets. It reduces the risk of overfitting and provides a more reliable estimate of model performance.\n\nModel Reliability - cross-validation helps you gauge the reliability of your model by testing it on different subsets of your data. This way, you reduce the risk of overfitting and ensure that your model has good performance across various datasets.\nPerformance Metrics - it provides you with more robust performance metrics by averaging the results from different folds. This gives you a better understanding of your model’s capabilities.\nReal-World Readiness - in the real world, the data your model encounters can vary. Cross-validation ensures that your model is not just tailored to one specific dataset but is generalized for better real-world application.\n\n\n# Basic parameter tuning\nfitControl <- trainControl(\n  ## 10-fold CV\n  method = \"repeatedcv\",\n  number = 10,\n  ## repeated ten times\n  repeats = 10)\n\n\n# Create a model\nglm_model_2 <- train(HeartDisease ~ ., \n                     data = trainData, \n                     method = \"glm\", \n                     family = \"binomial\", \n                     metric = \"Accuracy\", \n                     trControl = fitControl)\nglm_model_2\n\nGeneralized Linear Model \n\n213 samples\n  8 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 192, 192, 193, 192, 192, 191, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7701905  0.5336502\n\nsummary(glm_model_2)\n\n\nCall:\nNULL\n\nCoefficients:\n                             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  2.236059   2.614243   0.855 0.392365    \nAge                          0.017221   0.025332   0.680 0.496629    \nSexMale                      1.907153   0.473389   4.029 5.61e-05 ***\n`ChestPainAtypical angina`  -1.831258   0.551094  -3.323 0.000891 ***\n`ChestPainNon-anginal pain` -1.867625   0.464303  -4.022 5.76e-05 ***\n`ChestPainTypical angina`   -2.323870   0.715857  -3.246 0.001169 ** \nBP                           0.483273   0.200360   2.412 0.015864 *  \nCholesterol                  0.005616   0.003792   1.481 0.138597    \nBloodSugarTRUE              -0.134908   0.482257  -0.280 0.779675    \nMaximumHR                   -0.035188   0.011113  -3.166 0.001543 ** \nExerciseInducedAnginaYes     0.584593   0.435873   1.341 0.179855    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 293.92  on 212  degrees of freedom\nResidual deviance: 189.63  on 202  degrees of freedom\nAIC: 211.63\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n# Making predictions\npredictions_2 <- predict(glm_model_2, testData)\n\n# Evaluating the model\nconfusion_2 <- confusionMatrix(predictions_2, testData$HeartDisease)\nprint(confusion_2)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  44  10\n       Yes  5  31\n                                        \n               Accuracy : 0.8333        \n                 95% CI : (0.74, 0.9036)\n    No Information Rate : 0.5444        \n    P-Value [Acc > NIR] : 7.067e-09     \n                                        \n                  Kappa : 0.6606        \n                                        \n Mcnemar's Test P-Value : 0.3017        \n                                        \n            Sensitivity : 0.8980        \n            Specificity : 0.7561        \n         Pos Pred Value : 0.8148        \n         Neg Pred Value : 0.8611        \n             Prevalence : 0.5444        \n         Detection Rate : 0.4889        \n   Detection Prevalence : 0.6000        \n      Balanced Accuracy : 0.8270        \n                                        \n       'Positive' Class : No            \n                                        \n\n\n\nTASK:\nWhat if we choose only Boruta selected features?\n\n# Create a model\nglm_model_Boruta <- train(HeartDisease ~ ., \n                          data = trainData[, c(\"HeartDisease\", selected_var)], \n                          method = \"glm\", \n                          family = \"binomial\", \n                          metric = \"Accuracy\", \n                          trControl = fitControl)\nglm_model_Boruta\n\nGeneralized Linear Model \n\n213 samples\n  6 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 192, 191, 193, 192, 191, 191, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7698874  0.5345274\n\nsummary(glm_model_Boruta)\n\n\nCall:\nNULL\n\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  3.02104    2.52150   1.198 0.230873    \nAge                          0.02190    0.02487   0.881 0.378533    \nSexMale                      1.71767    0.44505   3.859 0.000114 ***\n`ChestPainAtypical angina`  -1.78515    0.54382  -3.283 0.001029 ** \n`ChestPainNon-anginal pain` -1.92352    0.45827  -4.197  2.7e-05 ***\n`ChestPainTypical angina`   -2.32802    0.69986  -3.326 0.000880 ***\nBP                           0.47159    0.19749   2.388 0.016944 *  \nMaximumHR                   -0.03228    0.01062  -3.040 0.002369 ** \nExerciseInducedAnginaYes     0.63108    0.42565   1.483 0.138178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 293.92  on 212  degrees of freedom\nResidual deviance: 191.88  on 204  degrees of freedom\nAIC: 209.88\n\nNumber of Fisher Scoring iterations: 5\n\n# Making predictions\npredictions_Boruta <- predict(glm_model_Boruta, testData)\n\n# Evaluating the model\nconfusion_Boruta <- confusionMatrix(predictions_Boruta, testData$HeartDisease)\nprint(confusion_Boruta)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  44  11\n       Yes  5  30\n                                          \n               Accuracy : 0.8222          \n                 95% CI : (0.7274, 0.8948)\n    No Information Rate : 0.5444          \n    P-Value [Acc > NIR] : 2.84e-08        \n                                          \n                  Kappa : 0.6373          \n                                          \n Mcnemar's Test P-Value : 0.2113          \n                                          \n            Sensitivity : 0.8980          \n            Specificity : 0.7317          \n         Pos Pred Value : 0.8000          \n         Neg Pred Value : 0.8571          \n             Prevalence : 0.5444          \n         Detection Rate : 0.4889          \n   Detection Prevalence : 0.6111          \n      Balanced Accuracy : 0.8148          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\nTASK:\nTry repeating the same steps for other methods, e.g. k-nearest neighbors (method = “knn”), support vector machine (method = “svmLinear”), random forest (method = “rf”), bagging (method = “treebag”).\nCheck all the possible models here.\n\n\nTASK:\nFind out how to plot AUC (Area Under the Curve) for your test data. AUC represents the overall performance of a binary classification model based on the area under its ROC (Receiver Operating Characteristic) curve. A higher AUC value indicates better model performance, reflecting a greater ability to distinguish between classes. Essentially, it’s a single metric summarizing how well a model can differentiate between positive and negative instances.\n*note: check pROC package"
  },
  {
    "objectID": "Introduction_to_ML.html#convert-categorical-variables-to-factor",
    "href": "Introduction_to_ML.html#convert-categorical-variables-to-factor",
    "title": "Introduction to machine learning",
    "section": "Convert categorical variables to factor",
    "text": "Convert categorical variables to factor\n\nheart_disease[, Sex := as.factor(Sex)]\nheart_disease[, ChestPain := as.factor(ChestPain)]\nheart_disease[, BloodSugar := as.factor(BloodSugar)]\nheart_disease[, ExerciseInducedAngina := as.factor(ExerciseInducedAngina)]\nheart_disease[, HeartDisease := as.factor(HeartDisease)]\n\nstr(heart_disease)\n\nClasses 'data.table' and 'data.frame':  303 obs. of  10 variables:\n $ rownames             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Age                  : int  63 67 67 37 41 56 62 57 63 53 ...\n $ Sex                  : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 1 1 2 2 ...\n $ ChestPain            : Factor w/ 4 levels \"Asymptomatic\",..: 4 1 1 3 2 2 1 1 1 1 ...\n $ BP                   : int  145 160 120 130 130 120 140 120 130 140 ...\n $ Cholesterol          : int  233 286 229 250 204 236 268 354 254 203 ...\n $ BloodSugar           : Factor w/ 2 levels \"FALSE\",\"TRUE\": 2 1 1 1 1 1 1 1 1 2 ...\n $ MaximumHR            : int  150 108 129 187 172 178 160 163 147 155 ...\n $ ExerciseInducedAngina: Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 2 1 2 ...\n $ HeartDisease         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 2 1 2 2 ...\n - attr(*, \".internal.selfref\")=<externalptr>"
  }
]